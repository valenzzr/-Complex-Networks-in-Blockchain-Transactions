# =====================================================
# Ethereum Transaction Network Analysis (Final Version)
# Complexity Science Final Project
# =====================================================

import requests
import pandas as pd
import numpy as np
import networkx as nx
import matplotlib
matplotlib.use("Agg")  # éäº¤äº’å¼åç«¯ï¼šä¸å¼¹çª—ï¼Œç›´æ¥ä¿å­˜æˆ PNG
import matplotlib.pyplot as plt
from sklearn.ensemble import IsolationForest
from tqdm import tqdm
from datetime import datetime

# =========================
# 0. CONFIGURATION
# =========================

API_KEY = "GEAB5WHJYW76B5SFUUVGQR2FE85755BYUH"  # <--- æ¢æˆä½ è‡ªå·±çš„ Etherscan API key

# é«˜æ´»è·ƒ/é«˜å¯è§åº¦çš„å·²çŸ¥é’±åŒ…åœ°å€ï¼Œäº¤æ˜“ä¸°å¯Œï¼ˆå…¬å¼€ä¿¡æ¯ï¼‰
seed_addresses_level1 = [
    "0x742d35Cc6634C0532925a3b844Bc454e4438f44e",  # Binance hot wallet
    "0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045",  # Vitalik Buterin
    "0xE592427A0AEce92De3Edee1F18E0157C05861564",  # Uniswap V3 Router
    "0x564286362092D8e7936f0549571a803B203aAceD",  # Tether Treasury
]

MAX_TX = 3000         # æ¯ä¸ªåœ°å€æœ€å¤šæŠ“å¤šå°‘ç¬”äº¤æ˜“
DO_SECOND_HOP = True  # True = æŠ“äºŒè·³ï¼ˆæ‰©å±•å¯¹æ‰‹æ–¹åœ°å€ï¼‰ï¼›False = åªæŠ“ä¸Šè¿°4ä¸ªç§å­


# =========================
# 1. ETHERSCAN DATA FETCH
# =========================

def fetch_transactions_for_address(address, api_key, max_tx=MAX_TX):
    """
    ç”¨ Etherscan V2 API è·å–æŸä¸ªåœ°å€çš„äº¤æ˜“å†å²ï¼ˆä¸»ç½‘ï¼‰ã€‚
    API å½¢å¦‚:
      https://api.etherscan.io/v2/api?chainid=1&module=account&action=txlist&address=...
    è¿”å›åˆ—: from, to, value(ETH), timeStamp(datetime), hash
    """
    url = (
        "https://api.etherscan.io/v2/api"
        f"?chainid=1&module=account&action=txlist"
        f"&address={address}"
        f"&startblock=0&endblock=9999999999"
        f"&page=1&offset={max_tx}"
        f"&sort=asc"
        f"&apikey={api_key}"
    )

    resp = requests.get(url)
    data = resp.json()

    # status == "1" è¡¨ç¤ºæˆåŠŸï¼›å¦åˆ™å¯èƒ½æ˜¯æ²¡äº¤æ˜“/é€Ÿç‡é™åˆ¶/é”™è¯¯
    if data.get("status") == "1" and "result" in data:
        df = pd.DataFrame(data["result"])
        if df.empty:
            return pd.DataFrame(columns=["from", "to", "value", "timeStamp", "hash"])

        keep_cols = ["from", "to", "value", "timeStamp", "hash"]
        df = df[keep_cols].copy()

        # wei -> ETH
        df["value"] = df["value"].astype(float) / 1e18

        # timestamp -> pandas datetime
        # å…ˆastype(int)å¯ä»¥é¿å… pandas çš„ future warning
        df["timeStamp"] = pd.to_datetime(df["timeStamp"].astype(int), unit="s")

        # ä¸¢æ‰æ²¡æœ‰ to çš„å¥‡æ€ª internal è®°å½•
        df = df[(df["from"] != "") & (df["to"] != "")]
        return df

    return pd.DataFrame(columns=["from", "to", "value", "timeStamp", "hash"])


print("=== Fetching seed addresses (1-hop) ===")
level1_list = []
for addr in tqdm(seed_addresses_level1):
    df_addr = fetch_transactions_for_address(addr, API_KEY)
    df_addr["seed_source"] = addr  # æ ‡è®°æ¥æºï¼ˆè°å¼•è¿›çš„è¿™ä¸ªäº¤æ˜“ï¼‰
    level1_list.append(df_addr)

df_level1 = pd.concat(level1_list, ignore_index=True).drop_duplicates(subset=["hash"])


# ---------- äºŒè·³æ‰©å±• ----------
if DO_SECOND_HOP:
    print("=== Selecting top neighbor addresses for 2-hop expansion ===")

    # ä»ä¸€è·³äº¤æ˜“é‡Œæ”¶é›†æ‰€æœ‰å‡ºç°è¿‡çš„åœ°å€ï¼ˆfrom / toï¼‰
    neighbors_all = pd.concat(
        [df_level1["from"], df_level1["to"]],
        ignore_index=True
    )

    # å‡ºç°æ¬¡æ•°æœ€é«˜çš„åœ°å€å¾€å¾€æ˜¯â€œæœ€æ´»è·ƒ/æœ€ä¸­å¿ƒçš„å¯¹æ‰‹æ–¹â€
    # æˆ‘ä»¬æŠ“è¿™äº›åœ°å€çš„äº¤æ˜“å†å²ï¼Œç½‘ç»œä¼šå˜å¾—æ˜æ˜¾æ›´å¤§
    top_neighbors = (
        neighbors_all.value_counts()
        .head(30)          # å–å‰30ä¸ªæœ€é¢‘ç¹å¯¹æ‰‹æ–¹
        .index
        .tolist()
    )

    level2_list = []
    for addr in tqdm(top_neighbors):
        # é¿å…é‡å¤æŠ“ç§å­åœ°å€
        if addr in seed_addresses_level1:
            continue
        df_addr = fetch_transactions_for_address(addr, API_KEY)
        df_addr["seed_source"] = addr
        level2_list.append(df_addr)

    if len(level2_list) > 0:
        df_level2 = pd.concat(level2_list, ignore_index=True).drop_duplicates(subset=["hash"])
    else:
        df_level2 = pd.DataFrame(columns=df_level1.columns)
else:
    df_level2 = pd.DataFrame(columns=df_level1.columns)

# åˆå¹¶æ‰€æœ‰æŠ“åˆ°çš„äº¤æ˜“
df_all = pd.concat([df_level1, df_level2], ignore_index=True).drop_duplicates(subset=["hash"])

print(f"Total unique transactions collected: {len(df_all)}")

all_nodes = pd.unique(
    pd.concat([df_all["from"], df_all["to"]], ignore_index=True)
)
print(f"Unique addresses involved: {len(all_nodes)}")


# ä¿å­˜äº¤æ˜“æ•°æ®åˆ° CSVï¼Œç”¨æ—¶é—´æˆ³ä¿è¯ä¸ä¼šå’Œæ‰“å¼€çš„æ—§æ–‡ä»¶å†²çª
ts = datetime.now().strftime("%Y%m%d_%H%M%S")
csv_name = f"ethereum_transactions_{ts}.csv"
df_all.to_csv(csv_name, index=False)
print(f"ğŸ’¾ Saved raw data to {csv_name}")


# =========================
# 2. BUILD DIRECTED GRAPH
# =========================

print("\n=== Building transaction graph ===")
G = nx.DiGraph()

# æ¯æ¡äº¤æ˜“å˜æˆä¸€æ¡æœ‰å‘è¾¹ (from -> to)ï¼Œè¾¹çš„æƒé‡ç´¯è®¡è½¬è´¦ETHé‡‘é¢
for _, row in df_all.iterrows():
    s, t, v = row["from"], row["to"], row["value"]
    if s == t:
        continue
    if G.has_edge(s, t):
        G[s][t]["weight"] += v
        G[s][t]["count"] = G[s][t].get("count", 1) + 1
    else:
        G.add_edge(s, t, weight=v, count=1)

print(f"Graph nodes: {G.number_of_nodes()}, edges: {G.number_of_edges()}")


# =========================
# 3. STATIC NETWORK ANALYSIS
# =========================

print("\n=== Computing network metrics ===")
deg_total = dict(G.degree())       # èŠ‚ç‚¹çš„æ€»åº¦æ•° (å…¥åº¦+å‡ºåº¦)
deg_in    = dict(G.in_degree())    # å…¥åº¦
deg_out   = dict(G.out_degree())   # å‡ºåº¦

# PageRank: èµ„é‡‘æµç½‘ç»œé‡Œâ€œé‡è¦æ€§é«˜/è¢«æŒ‡å‘å¤š/è·¯å¾„ä¸­è½¬å¤šâ€çš„èŠ‚ç‚¹
pagerank = nx.pagerank(G, alpha=0.85)

# èšç±»ç³»æ•°ï¼šæˆ‘ä»¬æŠŠå›¾è½¬æˆæ— å‘åæµ‹ clustering
clustering = nx.clustering(G.to_undirected())

avg_deg   = np.mean(list(deg_total.values()))
avg_clust = np.mean(list(clustering.values()))

print(f"Average degree: {avg_deg:.4f}")
print(f"Average clustering coefficient: {avg_clust:.4f}")

# PageRank Top-10 èŠ‚ç‚¹ï¼ˆå¾€å¾€æ˜¯äº¤æ˜“æ‰€/DeFiè·¯ç”±/ç¨³å®šå¸é‡‘åº“ç­‰å…³é”®è§’è‰²ï¼‰
print("\nTop 10 wallets by PageRank:")
top_pr = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]
for i, (node, val) in enumerate(top_pr, 1):
    print(f"{i}. {node} | PR={val:.6f}")

# å·¨å‹è¿é€šåˆ†é‡ (giant component)ï¼šæŠŠå›¾çœ‹æˆæ— å‘å›¾åï¼Œæœ€å¤§çš„è¿é€šå—å å¤šå°‘æ¯”ä¾‹
UG = G.to_undirected()
components = sorted(nx.connected_components(UG), key=len, reverse=True)
if len(components) > 0:
    giant_nodes = components[0]
    Gc = G.subgraph(giant_nodes)
    frac_gc = len(Gc.nodes()) / G.number_of_nodes()
    print(f"\nGiant component fraction: {frac_gc:.2%}")
else:
    frac_gc = 0.0
    print("\nGiant component fraction: N/A (empty graph?)")


# =========================
# 4. DEGREE DISTRIBUTION (SCALE-FREENESS)
# =========================

print("\n=== Saving degree distribution figure ===")
deg_arr = np.array(list(deg_total.values()))

plt.figure(figsize=(8,5))
plt.hist(deg_arr, bins=100, log=True)
plt.xlabel("Degree k")
plt.ylabel("Frequency (log scale)")
plt.title("Degree Distribution of Ethereum Transaction Network")
plt.tight_layout()
plt.savefig("figure_degree_distribution.png", dpi=300, bbox_inches="tight")
plt.close()

# å¯é€‰ï¼šæ‹Ÿåˆå¹‚å¾‹ï¼Œæ˜¾ç¤ºé•¿å°¾ï¼ˆä¸­å¿ƒåŒ–çš„é‡‘èä¸­æ¢ï¼‰
try:
    import powerlaw
    fit = powerlaw.Fit(deg_arr[deg_arr > 0])
    fit.plot_pdf(label="Empirical")
    fit.power_law.plot_pdf(linestyle="--", label="Power-law fit")
    plt.legend()
    plt.title(f"Power-law exponent Î± â‰ˆ {fit.alpha:.2f}")
    plt.tight_layout()
    plt.savefig("figure_powerlaw_fit.png", dpi=300, bbox_inches="tight")
    plt.close()
except ImportError:
    print("powerlaw not installed; skip power-law plot")


# =========================
# 5. CENTRALIZATION (GINI)
# =========================

print("\n=== Measuring centralization (Gini) ===")
def gini(x):
    x = np.sort(np.array(x))
    n = len(x)
    if n == 0:
        return 0.0
    cumx = np.cumsum(x)
    return (n + 1 - 2*np.sum(cumx)/cumx[-1]) / n

gini_coeff = gini(list(deg_total.values()))
print(f"Gini coefficient of node degree: {gini_coeff:.3f}")
# Gini è¶Šé«˜ -> è¿æ¥åº¦è¶Šé›†ä¸­åœ¨æå°‘æ•°èŠ‚ç‚¹æ‰‹ä¸­ -> èµ„é‡‘æµè¶Šâ€œä¸­å¿ƒåŒ–â€


# =========================
# 6. ANOMALY DETECTION
# =========================

print("\n=== Anomaly detection (IsolationForest) ===")
nodes_list = list(G.nodes())
X = np.array([
    [deg_total[n], deg_in[n], deg_out[n], pagerank[n]]
    for n in nodes_list
])

clf = IsolationForest(contamination=0.02, random_state=42)
labels = clf.fit_predict(X)

# IsolationForest è¾“å‡ºï¼š-1 = å¼‚å¸¸ï¼Œ1 = æ­£å¸¸
anomalies = [nodes_list[i] for i, lab in enumerate(labels) if lab == -1]

print(f"Detected {len(anomalies)} anomalous wallets (~2%)")
print("Top suspicious / high-impact wallets:")
for a in anomalies[:10]:
    print(" -", a)

# è¿™äº›å¾€å¾€æ˜¯ï¼š
# - èµ„é‡‘ä¸­è½¬/æ´—é’±æ¢çº½
# - å¤§å‹åˆ†å‘åˆçº¦/ç©ºæŠ•/ICOåˆçº¦
# - äº¤æ˜“æ‰€æ¸…ç®—/çƒ­é’±åŒ…
# - æ”»å‡»/è¯ˆéª—é›†èµ„åœ°å€ (çŸ­æ—¶é—´æ”¶åˆ°å¤§é‡å°é¢è½¬è´¦)


# =========================
# 7. TEMPORAL DYNAMICS
# =========================

print("\n=== Temporal dynamics ===")

# æŠŠäº¤æ˜“æŒ‰å¤©èšåˆï¼Œç ”ç©¶æ´»è·ƒåº¦å’Œè¿é€šæ€§çš„æ¼”åŒ–
df_all["date"] = df_all["timeStamp"].dt.date
dates_sorted = sorted(df_all["date"].unique())

# (1) æ¯å¤©çš„äº¤æ˜“æ•° & æ¯å¤©å‚ä¸è¿‡äº¤æ˜“çš„å”¯ä¸€åœ°å€æ•°
daily_tx_count = df_all.groupby("date").size()
daily_unique_addr = (
    df_all.groupby("date")[["from", "to"]]
    .nunique()
    .sum(axis=1)
)

plt.figure(figsize=(10,5))
plt.plot(daily_tx_count.index, daily_tx_count.values, label="Transactions/day")
plt.plot(daily_unique_addr.index, daily_unique_addr.values, label="Unique addresses/day")
plt.legend()
plt.title("Temporal Activity in Sampled Ethereum Subnetwork")
plt.xlabel("Date")
plt.ylabel("Count")
plt.tight_layout()
plt.savefig("figure_temporal_activity.png", dpi=300, bbox_inches="tight")
plt.close()

# (2) Giant component fraction over time
# æ€è·¯ï¼šéšç€æ—¶é—´å¾€å‰æ¨è¿›ï¼ŒæŠŠæ¯å¤©æ–°å¢çš„äº¤æ˜“ä¸æ–­åŠ è¿›ä¸€å¼ "ç´¯ç§¯å›¾"
# ç„¶åæµ‹è¿™å¼ å›¾çš„ giant component å æ¯” (ç½‘ç»œå‡èšç¨‹åº¦)
gc_ratio_time = []
Gt = nx.DiGraph()

for d in dates_sorted:
    todays_rows = df_all[df_all["date"] == d]
    for _, row in todays_rows.iterrows():
        s, t, v = row["from"], row["to"], row["value"]
        if s == t:
            continue
        if Gt.has_edge(s, t):
            Gt[s][t]["weight"] += v
        else:
            Gt.add_edge(s, t, weight=v)

    Ug2 = Gt.to_undirected()
    comps2 = list(nx.connected_components(Ug2))
    if len(comps2) > 0:
        biggest = max(comps2, key=len)
        frac = len(biggest) / max(1, Gt.number_of_nodes())
    else:
        frac = 0.0
    gc_ratio_time.append(frac)

plt.figure(figsize=(10,5))
plt.plot(dates_sorted, gc_ratio_time, marker="o", linewidth=1)
plt.title("Giant Component Fraction Over Time")
plt.xlabel("Date")
plt.ylabel("Giant component fraction")
plt.tight_layout()
plt.savefig("figure_giant_component_over_time.png", dpi=300, bbox_inches="tight")
plt.close()


# =========================
# 8. VISUALIZATION SNAPSHOTS
# =========================

# 8a. éšæœºé‡‡æ ·å­å›¾ (å¯èƒ½å¾ˆç¨€ç–ï¼Œä½œä¸ºâ€œéšæœºå±€éƒ¨è§†è§’â€)
print("\n=== Exporting random subgraph (200 nodes) ===")

if G.number_of_nodes() > 200:
    sample_nodes = np.random.choice(list(G.nodes()), 200, replace=False)
else:
    sample_nodes = list(G.nodes())

H_random = G.subgraph(sample_nodes)

pos_rand = nx.spring_layout(H_random, k=0.2, seed=0)
plt.figure(figsize=(8,8))
nx.draw_networkx_nodes(H_random, pos_rand, node_size=40)
nx.draw_networkx_edges(H_random, pos_rand, alpha=0.3, arrows=False)
plt.title("Ethereum Transaction Subgraph (sampled nodes)")
plt.axis("off")
plt.tight_layout()
plt.savefig("figure_subgraph_random.png", dpi=300, bbox_inches="tight")
plt.close()

# æ³¨æ„ï¼šå¦‚æœç½‘ç»œæ˜¯é«˜åº¦hub-and-spokeå‹ï¼ŒéšæœºæŠ½æ ·ç»å¸¸åªæŠ½åˆ°â€œå¶å­èŠ‚ç‚¹â€ä¸”å‡ ä¹ä¸äº’ç›¸è¿æ¥ï¼Œ
# æ‰€ä»¥è¿™å¼ å›¾å¾ˆå¯èƒ½åªæ˜¯ä¸€åœˆç‚¹ã€‚è¿™æœ¬èº«å°±æ˜¯ä¸€ä¸ªç°è±¡ï¼šå¤§å¤šæ•°åœ°å€å‡ ä¹åªè¿å‘ä¸­å¿ƒåŒ–hubã€‚


# 8b. ä»¥ PageRank æœ€é«˜çš„åœ°å€ä¸ºä¸­å¿ƒï¼Œç”»å®ƒçš„1è·³egoç½‘ç»œ
print("=== Exporting ego network of top hub ===")
hub_node = max(pagerank.items(), key=lambda x: x[1])[0]
print(f"Hub node for ego visualization: {hub_node}")

ego_nodes = set([hub_node])
ego_nodes.update(G.predecessors(hub_node))
ego_nodes.update(G.successors(hub_node))

H_ego = G.subgraph(ego_nodes).copy()

# èŠ‚ç‚¹å¤§å°ç”¨ PageRank ä½“ç°â€œé‡è¦æ€§â€
node_sizes = [300 + 2000 * pagerank.get(n, 0) for n in H_ego.nodes()]

pos_ego = nx.spring_layout(H_ego, k=0.3, seed=1)
plt.figure(figsize=(8,8))
nx.draw_networkx_nodes(
    H_ego,
    pos_ego,
    node_size=node_sizes,
    node_color="lightblue",
    edgecolors="k",
    linewidths=0.5,
)
nx.draw_networkx_edges(
    H_ego,
    pos_ego,
    alpha=0.3,
    arrows=True,
    arrowsize=10,
    width=0.5,
)
plt.title("Ego Network of Top-PageRank Hub")
plt.axis("off")
plt.tight_layout()
plt.savefig("figure_hub_ego_network.png", dpi=300, bbox_inches="tight")
plt.close()

# è§£é‡Šï¼šè¿™å¼ hub egoå›¾å°±æ˜¯â€œä¸­å¿ƒåŒ–èµ„é‡‘ä¸­æ¢ + å®ƒçš„æ‰€æœ‰ä¸€è·³è¿æ¥â€ã€‚
# è¿™é€šå¸¸èƒ½ç”»å‡ºâ€˜å¤ªé˜³æ”¾å°„çŠ¶/èœ˜è››â€™ç»“æ„ï¼Œæ˜¯å±•ç¤ºå»ä¸­å¿ƒåŒ–ç³»ç»Ÿé‡Œâ€œä¸­å¿ƒåŒ–æµåŠ¨æ¢çº½â€çš„æœ€å¥½è¯æ®ã€‚


# 8c. å¯¼å‡ºå…¨å›¾ç»™ Gephi åšé«˜è´¨é‡å¯è§†åŒ–ï¼ˆåŠ›å¯¼å¸ƒå±€ã€ç€è‰²ã€æ ‡ç­¾ç­‰ï¼‰
nx.write_gexf(G, "ethereum_network.gexf")


# =========================
# 9. DONE
# =========================

print("\nAll figures saved:")
print(" - figure_degree_distribution.png")
print(" - figure_powerlaw_fit.png (if generated)")
print(" - figure_temporal_activity.png")
print(" - figure_giant_component_over_time.png")
print(" - figure_subgraph_random.png")
print(" - figure_hub_ego_network.png")
print(f"Raw tx data saved to {csv_name}")
print("Full graph exported to ethereum_network.gexf")
print("DONE.")
